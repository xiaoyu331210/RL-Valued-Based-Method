{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# Declare agent\n",
    "import sys\n",
    "# Add the subfolder to sys.path\n",
    "sys.path.append('./tennis')\n",
    "from unityagents import UnityEnvironment\n",
    "import agent\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(agent)\n",
    "importlib.reload(utils)\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"tennis/Tennis.app\")\n",
    "# env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Environment parameters\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "obs_dim = env_info.vector_observations.shape[1]\n",
    "print(\"state size: \", obs_dim)\n",
    "act_dim = brain.vector_action_space_size\n",
    "print(\"action size: \", act_dim)\n",
    "state_dim = obs_dim * num_agents\n",
    "\n",
    "# Initialize MAPPO\n",
    "mappo = agent.MAPPO(obs_dim, act_dim, state_dim, num_agents, gamma=0.99, clip_param=0.2, ppo_epochs=10, lr=1e-4,\n",
    "                 batch_size=512, gae_lambda=0.9, entropy_coef=0.02)\n",
    "buffer = utils.ReplayBufferMAPPO()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Training parameters\n",
    "max_episodes = 25000\n",
    "max_steps = 1000\n",
    "update_freq = 2048\n",
    "print_interval = 20          # Print stats every 10 episodes\n",
    "save_interval = 100          # Save model every 100 episodes\n",
    "solved_threshold = 2.0       # Consider solved when average score > 0.5\n",
    "\n",
    "# Track rewards\n",
    "episode_rewards = []\n",
    "moving_avg = []\n",
    "moving_var = []  # New variance tracking\n",
    "best_avg_reward = -np.inf\n",
    "reward_window = deque(maxlen=100)  # For rolling average\n",
    "\n",
    "# Create directory for model saves\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Training loop\n",
    "for episode in range(max_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    episode_reward= np.zeros(num_agents)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        global_state = states.reshape(-1)\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "\n",
    "        # Get actions from all agents\n",
    "        for agent_id in range(num_agents):\n",
    "            action, log_prob = mappo.act(states[agent_id], agent_id)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "        # Environment step\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # Store experiences\n",
    "        episode_reward += rewards\n",
    "        next_global_state = next_states.reshape(-1)\n",
    "\n",
    "        for agent_id in range(num_agents):\n",
    "            buffer.add(\n",
    "                global_state=global_state,\n",
    "                individual_obs=states[agent_id],\n",
    "                action=actions[agent_id],\n",
    "                # reward=sum(rewards),\n",
    "                reward=rewards[agent_id],\n",
    "                next_global_state=next_global_state,\n",
    "                done=dones[agent_id],\n",
    "                log_prob=log_probs[agent_id],\n",
    "                agent_idx=agent_id\n",
    "            )\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        # Update when buffer is full\n",
    "        if len(buffer.rewards) >= update_freq:\n",
    "            mappo.update(buffer, episode, max_episodes)\n",
    "\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    # Update reward tracking\n",
    "    episode_rewards.append(max(episode_reward))\n",
    "    reward_window.append(max(episode_reward))\n",
    "    current_avg = np.mean(reward_window)\n",
    "    current_var = np.var(list(reward_window))  # Calculate variance\n",
    "    moving_avg.append(current_avg)\n",
    "    moving_var.append(current_var)  # Store variance\n",
    "    \n",
    "    # Save best model\n",
    "    if current_avg > (best_avg_reward + 0.05):\n",
    "        best_avg_reward = current_avg\n",
    "        mappo.save(episode=episode)\n",
    "    \n",
    "    # Print statistics\n",
    "    if (episode + 1) % print_interval == 0:\n",
    "        print(f\"Episode {episode + 1}, Current Reward: {max(episode_reward):.2f}, Average Reward (Last 100): {current_avg:.2f}, Reward Var (Last 100): {current_var:.2f}, Max Reward: {np.max(episode_rewards[-print_interval:]):.2f}, Min Reward: {np.min(episode_rewards[-print_interval:]):.2f}\")\n",
    "\n",
    "    # Early stopping if solved\n",
    "    if current_avg >= solved_threshold:\n",
    "        print(f\"\\nEnvironment solved in {episode + 1} episodes!\")\n",
    "        print(f\"Average Reward: {current_avg:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "mappo.save(episode=max_episodes)\n",
    "\n",
    "# Plotting the rewards (add this at the end)\n",
    "import matplotlib.pyplot as plt\n",
    "# Modified Plotting Section\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate standard deviation from variance for visualization\n",
    "std_dev = np.sqrt(np.array(moving_var))\n",
    "\n",
    "# Plot main reward curve and average\n",
    "plt.plot(episode_rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "plt.plot(moving_avg, linewidth=2, color='darkblue', label='100-Episode Average')\n",
    "\n",
    "# Plot variance as shaded area around the mean\n",
    "plt.fill_between(range(len(moving_avg)), \n",
    "                 np.array(moving_avg) - np.array(moving_var) * 2.,\n",
    "                 np.array(moving_avg) + np.array(moving_var) * 2.,\n",
    "                 color='skyblue', alpha=0.7, label='Variance')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training Progress with Reward Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reward_with_variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "# Declare agent\n",
    "import sys\n",
    "# Add the subfolder to sys.path\n",
    "sys.path.append('./tennis')\n",
    "from unityagents import UnityEnvironment\n",
    "import agent\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(agent)\n",
    "importlib.reload(utils)\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name=\"tennis/Tennis.app\")\n",
    "# env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Environment parameters\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "obs_dim = env_info.vector_observations.shape[1]\n",
    "print(\"state size: \", obs_dim)\n",
    "act_dim = brain.vector_action_space_size\n",
    "print(\"action size: \", act_dim)\n",
    "state_dim = obs_dim * num_agents\n",
    "\n",
    "# Initialize MAPPO\n",
    "mappo = agent.MAPPO(obs_dim, act_dim, state_dim, num_agents, gamma=0.99, clip_param=0.2, ppo_epochs=10, lr=1e-4,\n",
    "                 batch_size=512, gae_lambda=0.9, entropy_coef=0.02)\n",
    "# Load from checkpoint\n",
    "loaded_episode = mappo.load(\"tennis/mappo_models/mappo_checkpoint_ep25000.pth\", load_optimizer=False)\n",
    "print(f\"Resuming training from episode {loaded_episode}\")\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "states = env_info.vector_observations              # get the current state\n",
    "for _ in range(3000):\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    for agent_id in range(num_agents):\n",
    "        action, log_prob = mappo.act(states[agent_id], agent_id)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
