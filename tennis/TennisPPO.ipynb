{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono path[0] = '/Users/nuocheng/Desktop/Reinforcement_Learning/Reinforcement-Learning-Projects/tennis/Tennis.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/nuocheng/Desktop/Reinforcement_Learning/Reinforcement-Learning-Projects/tennis/Tennis.app/Contents/MonoBleedingEdge/etc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:  24\n",
      "action size:  2\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "# Declare agent\n",
    "import sys\n",
    "# Add the subfolder to sys.path\n",
    "sys.path.append('./tennis')\n",
    "from unityagents import UnityEnvironment\n",
    "import agent\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(agent)\n",
    "importlib.reload(utils)\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"tennis/Tennis.app\")\n",
    "# env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Environment parameters\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "obs_dim = env_info.vector_observations.shape[1]\n",
    "print(\"state size: \", obs_dim)\n",
    "act_dim = brain.vector_action_space_size\n",
    "print(\"action size: \", act_dim)\n",
    "state_dim = obs_dim * num_agents\n",
    "\n",
    "# Initialize MAPPO\n",
    "mappo = agent.MAPPO(obs_dim, act_dim, state_dim, num_agents, gamma=0.99, clip_param=0.2, ppo_epochs=10, lr=1e-4,\n",
    "                 batch_size=512, gae_lambda=0.9, entropy_coef=0.02)\n",
    "buffer = utils.ReplayBufferMAPPO()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to mappo_checkpoint_ep0.pth\n",
      "Episode 20, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 40, Current Reward: 0.00, Average Reward (Last 100): 0.02, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 60, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 80, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 100, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 120, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 140, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 160, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 180, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 200, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 220, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 240, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 260, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.09, Min Reward: 0.00\n",
      "Episode 280, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 300, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.00, Min Reward: 0.00\n",
      "Episode 320, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 340, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n",
      "Episode 360, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.10, Min Reward: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 380, Current Reward: 0.00, Average Reward (Last 100): 0.01, Reward Var (Last 100): 0.00, Max Reward: 0.00, Min Reward: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Environment step\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m env_info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m[brain_name]\n\u001b[1;32m     44\u001b[0m next_states \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mvector_observations\n\u001b[1;32m     45\u001b[0m rewards \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/site-packages/unityagents/environment.py:368\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrete\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vector_action[b]) \u001b[38;5;241m==\u001b[39m n_agent) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    358\u001b[0m                 (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    359\u001b[0m                     vector_action[b]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_size \u001b[38;5;241m*\u001b[39m n_agent)):\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a mismatch between the provided action and environment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms expectation: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe brain \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m action(s), but was provided: \u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_type,\n\u001b[1;32m    366\u001b[0m             \u001b[38;5;28mstr\u001b[39m(vector_action[b])))\n\u001b[0;32m--> 368\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_step_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/site-packages/unityagents/rpc_communicator.py:78\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[0;32m---> 78\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Training parameters\n",
    "max_episodes = 25000\n",
    "max_steps = 1000\n",
    "update_freq = 2048\n",
    "print_interval = 20          # Print stats every 10 episodes\n",
    "save_interval = 100          # Save model every 100 episodes\n",
    "solved_threshold = 2.0       # Consider solved when average score > 0.5\n",
    "\n",
    "# Track rewards\n",
    "episode_rewards = []\n",
    "moving_avg = []\n",
    "moving_var = []  # New variance tracking\n",
    "best_avg_reward = -np.inf\n",
    "reward_window = deque(maxlen=100)  # For rolling average\n",
    "\n",
    "# Create directory for model saves\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Training loop\n",
    "for episode in range(max_episodes):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    episode_reward= np.zeros(num_agents)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        global_state = states.reshape(-1)\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "\n",
    "        # Get actions from all agents\n",
    "        for agent_id in range(num_agents):\n",
    "            action, log_prob = mappo.act(states[agent_id], agent_id)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "        # Environment step\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # Store experiences\n",
    "        episode_reward += rewards\n",
    "        next_global_state = next_states.reshape(-1)\n",
    "\n",
    "        for agent_id in range(num_agents):\n",
    "            buffer.add(\n",
    "                global_state=global_state,\n",
    "                individual_obs=states[agent_id],\n",
    "                action=actions[agent_id],\n",
    "                # reward=sum(rewards),\n",
    "                reward=rewards[agent_id],\n",
    "                next_global_state=next_global_state,\n",
    "                done=dones[agent_id],\n",
    "                log_prob=log_probs[agent_id],\n",
    "                agent_idx=agent_id\n",
    "            )\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        # Update when buffer is full\n",
    "        if len(buffer.rewards) >= update_freq:\n",
    "            mappo.update(buffer, episode, max_episodes)\n",
    "\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    # Update reward tracking\n",
    "    episode_rewards.append(max(episode_reward))\n",
    "    reward_window.append(max(episode_reward))\n",
    "    current_avg = np.mean(reward_window)\n",
    "    current_var = np.var(list(reward_window))  # Calculate variance\n",
    "    moving_avg.append(current_avg)\n",
    "    moving_var.append(current_var)  # Store variance\n",
    "    \n",
    "    # Save best model\n",
    "    if current_avg > (best_avg_reward + 0.05):\n",
    "        best_avg_reward = current_avg\n",
    "        mappo.save(episode=episode)\n",
    "    \n",
    "    # Print statistics\n",
    "    if (episode + 1) % print_interval == 0:\n",
    "        print(f\"Episode {episode + 1}, Current Reward: {max(episode_reward):.2f}, Average Reward (Last 100): {current_avg:.2f}, Reward Var (Last 100): {current_var:.2f}, Max Reward: {np.max(episode_rewards[-print_interval:]):.2f}, Min Reward: {np.min(episode_rewards[-print_interval:]):.2f}\")\n",
    "\n",
    "    # Early stopping if solved\n",
    "    if current_avg >= solved_threshold:\n",
    "        print(f\"\\nEnvironment solved in {episode + 1} episodes!\")\n",
    "        print(f\"Average Reward: {current_avg:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "mappo.save(episode=max_episodes)\n",
    "\n",
    "# Plotting the rewards (add this at the end)\n",
    "import matplotlib.pyplot as plt\n",
    "# Modified Plotting Section\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate standard deviation from variance for visualization\n",
    "std_dev = np.sqrt(np.array(moving_var))\n",
    "\n",
    "# Plot main reward curve and average\n",
    "plt.plot(episode_rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "plt.plot(moving_avg, linewidth=2, color='darkblue', label='100-Episode Average')\n",
    "\n",
    "# Plot variance as shaded area around the mean\n",
    "plt.fill_between(range(len(moving_avg)), \n",
    "                 np.array(moving_avg) - np.array(moving_var) * 2.,\n",
    "                 np.array(moving_avg) + np.array(moving_var) * 2.,\n",
    "                 color='skyblue', alpha=0.7, label='Variance')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training Progress with Reward Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reward_with_variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono path[0] = '/Users/nuocheng/Desktop/Reinforcement_Learning/Reinforcement-Learning-Projects/tennis/Tennis.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/nuocheng/Desktop/Reinforcement_Learning/Reinforcement-Learning-Projects/tennis/Tennis.app/Contents/MonoBleedingEdge/etc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:  24\n",
      "action size:  2\n",
      "Loaded checkpoint from tennis/mappo_models/mappo_checkpoint_ep25000.pth\n",
      "Resuming training from episode 25000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m         actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m     43\u001b[0m         log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[0;32m---> 44\u001b[0m     env_info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m[brain_name]\n\u001b[1;32m     45\u001b[0m     states \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mvector_observations\n\u001b[1;32m     46\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/site-packages/unityagents/environment.py:368\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrete\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vector_action[b]) \u001b[38;5;241m==\u001b[39m n_agent) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    358\u001b[0m                 (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    359\u001b[0m                     vector_action[b]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_size \u001b[38;5;241m*\u001b[39m n_agent)):\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a mismatch between the provided action and environment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms expectation: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe brain \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m action(s), but was provided: \u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_brains[b]\u001b[38;5;241m.\u001b[39mvector_action_space_type,\n\u001b[1;32m    366\u001b[0m             \u001b[38;5;28mstr\u001b[39m(vector_action[b])))\n\u001b[0;32m--> 368\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_step_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/site-packages/unityagents/rpc_communicator.py:78\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[0;32m---> 78\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drlnd2/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "# Declare agent\n",
    "import sys\n",
    "# Add the subfolder to sys.path\n",
    "sys.path.append('./tennis')\n",
    "from unityagents import UnityEnvironment\n",
    "import agent\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(agent)\n",
    "importlib.reload(utils)\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name=\"tennis/Tennis.app\")\n",
    "# env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Environment parameters\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "obs_dim = env_info.vector_observations.shape[1]\n",
    "print(\"state size: \", obs_dim)\n",
    "act_dim = brain.vector_action_space_size\n",
    "print(\"action size: \", act_dim)\n",
    "state_dim = obs_dim * num_agents\n",
    "\n",
    "# Initialize MAPPO\n",
    "mappo = agent.MAPPO(obs_dim, act_dim, state_dim, num_agents, gamma=0.99, clip_param=0.2, ppo_epochs=10, lr=1e-4,\n",
    "                 batch_size=512, gae_lambda=0.9, entropy_coef=0.02)\n",
    "# Load from checkpoint\n",
    "loaded_episode = mappo.load(\"tennis/mappo_models/mappo_checkpoint_ep25000.pth\", load_optimizer=False)\n",
    "print(f\"Resuming training from episode {loaded_episode}\")\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "states = env_info.vector_observations              # get the current state\n",
    "for _ in range(3000):\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    for agent_id in range(num_agents):\n",
    "        action, log_prob = mappo.act(states[agent_id], agent_id)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
